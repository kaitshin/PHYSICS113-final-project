\documentclass[twocolumn,aps,prl]{revtex4-1} % For final two-column format
%\documentclass[twocolumn,aps,prl]{revtex4-1} % For double-spaced, single-column draft

\usepackage{graphicx}% Include figure files
\usepackage{amsmath,amssymb}
\usepackage{braket}
\usepackage{esvect}

% It may be convenient to define shortcut commands such as these:
\newcommand{\pd}{\partial}



\begin{document}

\title{Simulating the 2D Ising Model and Extensions}

\author{Newton Cheng and Kaitlyn Shin}
\affiliation{Department of Physics, Stanford University, Stanford, California 94305, USA}

\date{\today}
\begin{abstract}
In this paper, we review the basics of Markov chain Monte Carlo techniques, and then proceed to apply said techniques to simulate the 2D Ising model using the Metropolis and Wolff cluster algorithms. Using our simulation, we find evidence of a phase transition and critical behavior at a critical temperature of $T_c = 234123412$ and values of the critical exponents $\alpha = 2141$, $\beta = 234532$, and $\gamma = 2345234$. These results agree with the analytic values to $123421\%$. We then extend our work by applying our techniques to the Potts model and XY model. In all cases, we find evidence of critical behavior. We conclude by considering possible improvements to our simulations, as well as possible further avenues of exploration.
\end{abstract}

\maketitle
\section{Introduction}
Lattice systems have long been of interest because they provide a tractable microscopic description for macroscopic systems built from microscopic degrees of freedom. In particular, lattice systems are often able to exhibit phase transitions, providing a window into the microscopic details of such complex phenomena.

One of the simplest and most studied lattice systems is the Ising model, which has proved to be a useful model for describing ferromagnetism and generically phase transitions between magnetized and non-magnetized phases. The model also has several natural generalizations to more complex or realistic models, such as those with higher dimensions, more spin states, or modeling quantum statistical mechanics. Our goal in this paper will be to explore the properties of the 2D Ising model and some of its more complicated cousins via computer simulation.

We begin by reviewing the theoretical details of the 2D Ising model, as well as introducing our primary simulation tool, Markov chain Monte Carlo methods. We then perform simulations with two such methods, the Metropolis and Wolff cluster algorithms, to analyze the behavior of the model around the critical temperature for its phase transition. Because the 2D Ising model is so well-studied, we have access to a wealth of analytic results that we can check our simulation against; we will aim to reproduce the critical temperature and various critical exponents.





Note that we use natural units with $k_B = 1$. 


\section{2D Ising Model}
The 2D Ising model on a lattice is defined by the Hamiltonian:
\begin{equation}
H = -J\sum_{\braket{ij}}s_is_j - h\sum_i s_i
\end{equation}
where $s_i$ is the $i$th spin on the lattice, $\braket{ij}$ refers to nearest neighbors on the lattice, $J$ is the coupling strength between the spins, and $h$ is a coupling to an external magnetic field. Without loss of generality, we can take $s_i = \pm 1$ (just rescale $J$), and we will be concerned with the basic case of the square lattice. Notice that the energy is determined only by local properties of the total spin configuration, and the sign of $J$ determines if spins preferentially align or anti-align. When $J > 0$, the model is ferromagnetic, and $J < 0$ corresponds to an anti-ferromagnetic model. We will only be concerned with the case of no external magnetic field, so we set $h = 0$. 

The partition function is
\begin{equation}
Z = \sum_{\{s_i\}}e^{-\beta H}
\end{equation}
where $\{\sigma_i\}$ is the set of all $2^N$ spin configurations of the lattice with $N$ sites. Some quantities we will be interested in are the mean energy and the mean magnetization per spin:
\begin{align}
\braket{E} &= -J \left\langle\sum_{\braket{ij}}s_is_j\right\rangle \\
\braket{m} &= \frac{1}{N}\braket{M} = \frac{1}{N}\left\langle\sum_is_i\right\rangle 
\end{align}
We will also be interested in the specific heat per spin and the magnetic susceptibility:
\begin{align}
c &= \frac{1}{N}\frac{\pd \braket{E}}{\pd T} = \frac{\beta^2}{N}(\braket{E^2} - \braket{E}^2) \\
\chi &= \frac{1}{N}\frac{\pd \braket{M}}{\pd B} = \frac{\beta}{N} (\braket{M^2} - \braket{M}^2)
\end{align}

We can understand some of the properties of the model by examining the high and low temperature limits of the model with $N$ lattice sites. At low temperatures, the two ground states of fully aligned spins dominate the partition function. The energy per spin of an infinite lattice or lattice with periodic boundary conditions with coordination number $z$ is simply $E/N = -Jz/2$, so for our square lattice with $z = 4$, we have $E/N = -2J$. We also expect that the mean magnetization per spin should be $1$ or $-1$, as is the case for the two ground states.

At high temperatures, every spin configuration is equally likely, so the highest entropy configurations dominate. We therefore expect that there are approximately equal numbers of up and down spins, the mean energy per spin is 0, and that the magnetization $\braket{s}$ is 0. This can be analytically confirmed by performing a high temperature expansion of the partition function, upon which one finds that only closed paths on the lattice (in terms of active bonds) contribute. 

One of the primary motivations for studying the 2D Ising model is the presence of a phase transition; this is evident from the change of the order parameter $\braket{m}$ at high and low temperatures. The critical temperature can be analytically computed using the self-dual property of the 2D Ising model on the square lattice; every term in the partition function's high temperature expansion can be mapped in a unique fashion to the partition function's low temperature expansion. The temperature at the ``crossing point" can be computed to be $T_c/J = \frac{2}{\ln(\sqrt{2}+1)}$. As a check on our simulation later, we will compute various thermodynamic quantities of the system on both sides of the critical temperature, and check that we obtain the expected results.

We will also be interested in analyzing the critical behavior itself. The phase transition is second-order, with generic power-law divergences in the the specific heat, magnetization per spin, and magnetic susceptibility as a function of the reduced temperature $\tau = \frac{|T-T_c|}{T_c}$. The associated critical exponents can be analytically computed as $\alpha = 0$, $\beta = \frac{1}{8}$, and $\gamma = \frac{7}{4}$, respectively. Ideally, our simulation will be able to reproduce all 3 critical exponents.

\section{Markov Chain Monte Carlo}
In thermal systems, we are generally interested in the expectation value of some observable $Q$, rather than the precise state of the system. Generically, the expectation value can be computed with the system's partition function:
\begin{equation}
	\braket{Q} = \frac{\sum_{n}Q_ne^{-\beta E_n}}{\sum_{n}e^{-\beta E_n}}
\end{equation}
Beyond the absolute simplest models, analytically computing such a quantity is intractable. The idea behind using Monte Carlo techniques to simulate such systems is that one samples states according to some specified probability distribution, and then obtains an estimate of $\braket{Q}$ from the sample:
\begin{equation}
	Q_M = \frac{\sum^M_{n=1}Q_n\frac{e^{-\beta E_n}}{p_n}}{\sum^M_{n=1}\frac{e^{-\beta E_n}}{p_n}}
\end{equation}
One possible choice is to sample all possible states with equal probability, so $p_n = 1$ for all $n$. This is generally a poor choice, because outside of extremely high temperatures, there are relatively few states that dominate the partition function at any given temperature due to the exponential suppression of the Boltzmann factor, so most of the chosen sampled states in the estimator above will contribute nothing to the sum. Correspondingly, the estimate itself will be highly inaccurate unless one samples a large number of states.

A clever choice of probability distribution is the Boltzmann distribution itself $p_n = e^{-\beta E_n}/Z$, so that the dominant states are sampled most frequently, and we obtain a more accurate estimate while looking at fewer states. The estimator becomes
\begin{equation}
	Q_M = \frac{1}{M}\sum_{n=1}^MQ_n
\end{equation}
which is much more tractable. This general strategy is called importance sampling. The goal is then to develop a systematic method of selecting states according to the Boltzmann distribution.

The obvious choice is to randomly choose states, which we then accept with probability $e^{-\beta E_n}$. This runs into the same problem as before, where almost all states have a minuscule probability of acceptance. A better choice is to use a Markov process, which generically refers to a process by which a given state $X$ transitions to a state $Y$ with transition probability $P(X \to Y)$. The transition probabilities for a Markov process are constant, depend only on the initial state $X$ and candidate state $Y$, and sum to 1. Repeatedly applying such a process to evolve a system generates a Markov chain of states. The next step is to select the transition probabilities to reflect the Boltzmann distribution.

The fact that we hope to simulate a thermal system creates two constraints on the transition probabilities: ergodicity and detailed balance. Ergodicity is the requirement that any state be obtainable from any other state using the algorithm. This arises from the fact that all possible states for a system at any energy will appear with some non-zero probability in the Boltzmann distribution. Note that this does not mean every possible state for a system must be accessible with one Monte Carlo step, only that there must exist some possible (non-zero probability) path of steps to get from one state to any other state. Detailed balance is the requirement that the rate a system transitions from a state $X$ to a state $Y$ is equal to the rate that a system will transition from $Y$ to $X$, which we model as 
\begin{equation}
	p_XP(X \to Y) = p_YP(Y \to X)
\end{equation}

We want our sample probabilities $p_X,\ p_Y$ to satisfy the Boltzmann distribution, so we constrain the transition probabilities to satisfy:
\begin{equation}
\frac{P(X \to Y)}{P(Y \to X)} = \frac{p_Y}{p_X} = e^{-\beta(E_Y - E_X)}
\end{equation}

The total transition probability of moving from state $X$ to state $Y$ is the product of these two probabilities: $P(X \to Y) = g(X \to Y)A(X \to Y)$. The selection probability $g$ refers to the probability distribution from which we select a new state, and the acceptance ratio $A$ refers to the probability that we accept the new state and transition to it from our current state. An algorithm refers to a particular choice of the selection probability distribution and the acceptance ratios.


\section{Metropolis Algorithm}
\subsection{Theory}
The first algorithm we will use to simulate the 2D Ising model is perhaps the most historically important: the Metropolis algorithm, introduced by Nicolas Metropolis and his colleagues in 1953. This is also the most well-known example of a single-spin-flip algorithm, so-called because the state-to-state evolution occurs by flipping one spin each time. The selection probabilities of all states that differ from the current state by one spin are equal, and the selection probability of all other states are 0. More concretely, consider a lattice of $N$ spins in a configuration we call $X$. There are a set of spin configurations $\{S_i\}$ that differ from $X$ by one spin. Then the selection probability is
\begin{equation}
g(X \to Y) = \begin{cases}
\frac{1}{N},& \quad Y = S_i \\
0,& \quad \text{else}
\end{cases}
\end{equation}
Detailed balance then implies that the acceptance ratios must satisfy:
\begin{equation}
\frac{P(X \to Y)}{P(Y \to X)} = \frac{A(X \to Y)}{A(Y \to X)} = e^{-\beta(E_Y - E_X)}
\end{equation}

The simplest choice would be to simply set $A(X \to Y) = A_0e^{-\frac{1}{2}\beta(E_Y - E_X)}$ for some constant $A_0$, which turns out to be a maximum at $e^{-4\beta J}$ for the square lattice. Unfortunately, this is extremely slow and inefficient, because almost all acceptance probabilities are 0; in particular, when $E_Y - E_X > 0$, there is almost no chance of acceptance. 

The Metropolis algorithm solves this issue by offering an acceptance ratio:
\begin{equation}
A(X \to Y) = \begin{cases}
e^{-\beta(E_Y - E_X)}, & E_Y - E_X > 0 \\
1, & \text{else}
\end{cases}
\end{equation}
In other words, if the selected configuration has a lower energy than the current one, it is automatically accepted. Otherwise, it is accepted with a probability determined by the Boltzmann factor. This acceptance ratio manifestly obeys the detailed balance constraint, and has the advantage of being significant faster, because it spends far less time rejecting states than the simplest choice. Ergodicity is also clear with this algorithm, as it is clearly possible to reach any state from any other by systematically flipping individual spins.


\subsection{Implementation}
Our simulation is performed entirely in Python. For all simulations, we use units such that $J = 1$.

Taking our spins to have value $\pm 1$, we define a 2D array with elements that can take on $\pm 1$ to represent our array. We take the array to have periodic boundary conditions, so that we can neglect edge effects. The starting state for almost all of our simulations is the $T = 0$ state with all spins aligned. The other option is to use a $T = \infty$ state, where every spin has a $50/50$ probability of taking on either $+1$ or $-1$. 

Now we apply the Metropolis algorithm. To do this, we use a program that performs the following steps:
\begin{enumerate}
	\item Randomly select a spin on the given lattice
	\item Flip the spin and compute the change in energy of the lattice $\Delta E$
	\item If $\Delta E < 0$, accept the new state
	\item If $\Delta E > 0$, generate a random number $X$ from a uniform distribution $[0,1)$
	\item If $X < e^{-\beta \Delta E}$, accept the new state; otherwise reject the new state
\end{enumerate}
We may then iterate over this function $n$ times to perform $n$ timesteps of evolution. It is useful to introduce the time-unit \textit{sweep}, corresponding to $N$ timesteps, where $N$ is the number of lattice sites. The attempt frequency of flipping any particular spin is then independent of the lattice size, and we will use this as our preferred unit of time. Some pictures of the evolution of a lattice are shown in Fig.~\ref{fig:latevo}.
\begin{figure*}
	\includegraphics[width=\columnwidth]{"Lattice evolution".pdf}
	\caption{\label{fig:latevo}The evolution of a $100 \times 100$ square lattice at $T = 2.4$ via the Metropolis algorithm. The sequence is ordered from left to right, top to bottom. The two spin states $+1$ and $-1$ correspond to black and white, respectively. We have initialized the system in the $T = 0$ state, and plotted the system after 0, 1, 5, 10, 20, 50, 100, 500, and 1000 Monte Carlo sweeps.}
\end{figure*}

\subsection{Optimizations}
Because we will be interested in performing long running simulations across multiple temperatures, we would like to make the simulation as efficient as possible. Towards that end, we implement several optimizations.

First, we take advantage of the fact that the energy of a spin on the lattice is a local property of the lattice. Computing the energy of the entire lattice is generically a time-consuming process; the number of bonds on the lattice is $N_B = 2N$, so the energy will be given by a sum with that many terms. Once the number of iterations becomes large, this will be a highly expensive sum to compute at every step. Instead, we can directly compute the change in energy, independent of the total energy. The difference in energy between two states $X$ and $Y$ that differ by one spin $s_k$ is
\begin{align}
E_Y - E_X &= -J\sum_{\braket{ij}}s^Y_is^Y_j + J\sum_{\braket{ij}}s^X_is^X_j \\
&= -J\sum_{\braket{ik}}s^X_i(s^Y_k-s^X_k)
\end{align}
The change in energy is just a sum over the nearest neighbors to $s_k$. Then using the fact that $s^Y_k = -s_k^X$, we conclude
\begin{equation}
\Delta E = 2Js_k^X\sum_{\braket{ik}}^{}s_i^X
\end{equation}
This is just a sum over 4 terms and depends only on the state prior to flipping the spin. Evidently this is far more efficient than summing over the entire lattice at every step.

Given that we compute the change in energy at every step, we also have a way to efficiently measure the energy at every timestep: compute the energy of the initial lattice $E_i$, then just increment that number by the change in energy with every timestep $E_j = E_i + \Delta E$. This is far more efficient than computing the energy of the entire lattice at every step.

We may use a similar trick for keeping track of the magnetization of the lattice. Generically, we need to sum over the entire lattice, but we know that the change in magnetization of flipping, say, spin $s_k$ is simply $\Delta M = 2s_k$. Then we just compute the magnetization of the lattice at the beginning, and increment that value by $\Delta M$ every time a spin is flipped.

Because the change in energy is limited to 4 terms that are $\pm 2J$ (the change per bond), the entire spectrum of energy changes is simply $\{-8J, -4J, 0, 4J, 8J\}$. Then instead of computing the Boltzmann factor $e^{-\beta\Delta E}$ at every iteration, which is very computationally expensive due to the exponential, we can compute the factors once at the beginning of the simulation for a given temperature. Moreover, we do not even need to compute the Boltzmann factor for all the energy changes, just the ones that are positive. We then need only compute 2 exponentials, instead of $5n$, where $n$ is the number of iterations.

\subsection{Equilibration and Measurement}
Given that our initial lattice is at $T = 0$ or $T = \infty$, we have to allow the simulation to equilibrate at our desired temperature $T$ before taking any measurements. For example, it's clear that the $T = 0$ configuration is not an equilibrium configuration for $T = 100$, because several spins will flip within the first several iterations. We may also refer to Fig.~\ref{fig:latevo}: any measurements made in the first 6 states are not accurate with respect to the final (most accurate) state. For our simulation, we take equilibration this to mean that thermodynamic quantities whose mean values depend on the system temperature are relatively stable within a narrow range. This is purposefully a vague definition; we are ultimately more interested in the thermodynamic quantities of the state, not the specific state itself, and we expect thermal fluctuations about the mean. In our simulation, we consider 2 quantities: the energy per spin and magnetization per spin. An example of how these quantities will evolve over time is shown in Fig.~\ref{fig:equil}. 
\begin{figure}
	\includegraphics[width=\columnwidth]{"Equilibration".pdf}
	\caption{\label{fig:equil}Plots of the total energy and the magnetization per spin on a $100 \times 100$ square lattice at $T = 2.6$ initialized at $T = 0$ and evolved with the Metropolis algorithm. We see that after roughly 500 sweeps, the system is in a comfortable equilibrium, with the energy per spin hovering around $-1.05$ and the magnetization per spin around $0$. These plots were generated using the increment trick discussed in the optimization section.}
\end{figure}

We see that for the first part of the simulation, the energy and magnetization rapidly change; clearly any measurements taken at any point in that time range will be inaccurate. However, after roughly 500 sweeps, we see that the energy and magnetization become stable, oscillating around a mean value. Such behavior signals equilibration. In all our simulations, we first perform such a check to ensure that our system has equilibrated before beginning to take measurements. Moreover, because we are interested in a small range of temperatures, one can equilibrate beginning from the final state of the previous temperature. Because the states will presumably be fairly close, equilibration occurs faster.

We want to obtain mean values for various quantities like the energy. The natural way to do this is to take a series of measurements at the same temperature, and then find the arithmetic mean of the measurements. However, such a process only yields the true mean if each measurement is independent of the previous one. Because our simulation utilizes single-flip dynamics, it seems clear that measurements taken too soon together will be correlated. In an extreme case, one can imagine measuring the magnetization of the system after just one Monte Carlo step, so that the second measurement is either the same as the first one or differs by $\pm 2$, which is a very strong correlation. So ideally we need to wait long enough between measurements that the state we measure from is significantly different from the first.

To obtain the correlation time, we can, for example, compute the magnetization auto-correlation defined by
\begin{align}
\chi(t) = \int dt'[m(t')m(t'+t)-\braket{m}^2]
\end{align}
To get statistically independent measurements, we would ideally like to wait twice the correlation time $2\tau$, where $\tau$ is the time needed for the auto-correlation to drop by a factor of ${1}/{e}$. Unfortunately, we immediately run into the issue of time: $\tau$ is at least as large as 1 sweep (formally giving every spin the chance to flip), but usually larger. For practical purposes, we take measurements after every sweep; although our measurements are not entirely uncorrelated, we will be more interested in behavior across a temperature range than at a specific temperature, so the trend of the data will be more important than their specific values. We also rely on the fact that expect our data to fall within a relatively narrow range of values to begin with (this naturally becomes less accurate as fluctuations grow).

While the Metropolis algorithm is extremely efficient at very high and very low temperatures, the critical temperature region poses a major difficulty for two reasons: critical fluctuations and critical slowing down. As one approaches the critical temperature, the presence of spin clusters can cause significant statistical error and large fluctuations in the region of interest. One can somewhat mitigate this issue by approaching the critical temperature from below, but statistical errors will still tend to be large around the critical temperature. Formally, one deals with statistical errors by simply averaging over a larger sample size. However, as one approaches the critical temperature, the correlation time will generally scale as $\tau \propto \xi^z$, where $\xi$ is the correlation length and $z$ is the critical exponent and has been measured to be approximately $2.17$ for the Metropolis algorithm. At $T_c$, the correlation length is the size of the lattice itself (so diverges in the thermodynamic limit), implying that large numbers of measurements will take an impractical amount of time to obtain. Critical slowing down is an innate aspect of the Metropolis algorithm, so there is nothing much that can be done to deal with the issue. We will see that the Wolff algorithm is much more efficient around the critical temperature, because critical slowing down is much less pronounced.

\subsection{Error}
The error in our measurements is computed using the bootstrap method -- the benefit of this method is that it does not require knowing the correlation time, nor does it require truly independent measurements. Moreover, it also readily applies to direct measurements, like mean energy, and to functions of means, like the specific heat.

Using the specific heat as an example, the idea is to resample our total sample of measurements and compute the specific heat from that resample, then repeat the process several times. As an example, suppose we are given 100 measurements of the energy that we would like to use to compute the specific heat. Then we choose 100 values from the measurements with replacement, so that we may pick up duplicates, and compute the specific heat from those 100 values. We then repeat the process, and after several iterations, say 1000, the standard deviation of the bootstrap values approaches the true error:
\begin{equation}
\sigma^2 = \bar{c^2} - \bar{c}^2
\end{equation}
\subsection{Accuracy Verification}
As a first check, we want to make sure that the system behaves as we would expect at low and high temperature limits. In the low temperature limit, all the spins should be aligned so that the energy is at the minimum $E_0 = -2N$ and the magnitude of the magnetization is 1. In the high temperature limit, spins are randomly aligned so that both the energy and the magnetization are 0. Some examples of a lattice at different temperatures are shown in Fig.~\ref{fig:4temps}. 
\begin{figure}
	\includegraphics[width=\columnwidth]{"4 Temps".pdf}
	\caption{\label{fig:4temps} Four 100 $\times$ 100 lattices evolved with the Metropolis algorithm for 1000 sweeps at different temperatures. Moving left to right and top to bottom, the temperatures are $T = 0.01,\ 2,\ 3,\ 100$.}
\end{figure}

We see that we have the expected behavior: a progression of full alignment at very low temperatures, domain formation across the critical temperature, and fully random spins at high temperatures. Our implementation of the Metropolis algorithm is able to produce lattices that align with our physical intuitions for a lattice system at various temperatures.

A stronger check of the accuracy of simulation, and indeed our goal, is how well it can reproduce the expected critical behavior of the 2D Ising model. To do this, we explore a range of temperatures around the known critical temperature $T_c = \frac{2}{\ln(1+\sqrt{2})}J \approx 2.269J$ with $J = 1$. At every temperature value, we allow the system to equilibrate over $100L$ sweeps for a $L \times L$ lattice, and then proceed to make $100L$ measurements of the energy and magnetization, with each measurement separated by one sweep. The measurements can then be averaged to obtain the specific heat and magnetic susceptibility. All the plots of these values for every lattice we explored can be found in the \textbf{APPENDIX}.

There are several caveats to keep in mind about our results. The equilibration time value is somewhat arbitrarily chosen using the heuristic that it takes order $L^3$ steps to equilibrate, and is probably not enough for temperatures very close to the critical temperature. However, we would like to explore intermediate-sized lattices, so we opt to trade accuracy in the computation for accuracy in the system size. From looking at the various plots for different lattice sizes, increasing the lattice size also greatly improves the precision of our results and reduces the error on each data point. The inaccuracy of the simulation near the critical temperature is particularly pronounced for the smaller lattices because the critical fluctuations affect a larger fraction of the lattice, and extracting information about the critical behavior is close to impossible due to the scatter of the data. Finally, the finite temperature resolution size ultimately limits the precision of our results, so we cannot really trust our results beyond 3 significant figures. Critical slowing down makes the Metropolis algorithm highly impractical for exploring finer temperature resolutions around the critical temperature, and the finite size of the lattice means a finer temperature resolution is unlikely to provide enough information to offset the extreme time cost.

Using our values for the magnetization per spin and magnetic susceptibility, we perform a non-linear least squares fit to a function of the form
\begin{equation}
	f(x) = a\left(\frac{b-x}{b}\right)^c, \quad f'(x) = a'\left(\frac{x-b'}{b'}\right)^{c'}
\end{equation}
where the primed parameters are for fitting a curve above the critical temperature. We perform the fit using the SciPy \texttt{optimize.curve\_fit} function and use the \texttt{sigma} argument to manually input the errors computed using the bootstrap method. The fit parameters $b,b'$ and $c,c'$ correspond to the critical temperature and critical exponent, respectively. We can also fit the specific heat per spin to this form, but the analytic solution for the 2D Ising model implies a logarithmic divergence, rather than a power law. Furthermore, in the case of the specific heat, the maximum we see is actually at the critical temperature \textbf{CITE}. It is also for this reason that we are looking to explore the largest lattices we can that do not require excessive computational time. We also expect that the finite size of the lattice will affect the behavior of the various thermodynamic quantities around the phase transition; the divergences associated with the phase transition are only true divergences in the thermodynamic limit $N \to \infty$. It is also well-known that the critical exponents and critical temperature suffer from finite size effects. We have not attempted to analyze these effects carefully, but we keep them in mind when discussing the confidence in our results.


\subsection{Results}
The largest lattice we consider is a $50 \times 50$ lattice with measurements presented in Fig.~\ref{fig:IsingM}.
\begin{figure*}
	\includegraphics[width=\linewidth]{"Ising Measurements".pdf}
	\caption{\label{fig:IsingM}Plots showing the mean energy per spin, specific heat per spin, mean (absolute) magnetization per spin, magnetic susceptibility on a 50 $\times$ 50 lattice from $T = 1.6$ to $T = 2.9$ in increments of $\Delta T = .01$. The lines overlaid on the data are lines of best-of-fit determined using \texttt{optimize.curve\_fit}. The equilibration time at each temperature is 5000 sweeps and each data point is obtained from 5000 measurements. The error bars computed using the bootstrap method are too small to see.}
\end{figure*}

The first thing to note from the measurements is that we see evidence of critical behavior at roughly $T = 2.2$-$2.3$, where we expected the critical temperature to be based on analytic computations. The phase transition in the 2D Ising model is second-order, signaled by the order parameter (absolute magnetization per spin) dropping from 1 to 0 and characterized by divergences in the specific heat and susceptibility. We indeed see that the magnetization is very close to 1 below the critical temperature and is very close to 0 above the critical temperature. We also see peaks in the specific heat and susceptibility around the critical temperature. As expected, they remain finite throughout the simulation. Correspondingly, the step in the energy is not a true step with an infinite derivative, because $c \propto \frac{\pd E}{\pd T}$ is always finite.

In the region very close to the critical temperature, all 4 quantities see relatively significant scatter. This is most likely a result of critical slowing down. There is a strong chance that 5000 sweeps is not enough time to achieve equilibration in the critical temperature region; combined with critical fluctuations, this effect creates the scattering effect, even though we average over a large number of measurements. The main constraint on a longer equilibration time is a practical one: the time needed to truly equilibrate near the critical temperature for the Metropolis algorithm is simply too long to be efficient.

Despite such effects, we are still able to see how the quantities generally behave in the critical temperature region, and our lattice is large enough that we have enough precision to attempt to fit curves with reasonable confidence. By fitting to functions with the form in Eq. (20), we find values for the critical exponents as presented in Table~\ref{tab:crit}. We separately discuss the obtained values for the critical temperature below.
\begin{table}[b]%The best place to locate the table environment is directly after its first reference in text
	\caption{\label{tab:crit}Various critical exponents obtained from applying the Metropolis algorithm to a 50 $\times$ 50 grid. The uncertainties are obtained directly from the \texttt{curve\_fit} output covariance matrix .}
	\begin{ruledtabular}
		\begin{tabular}{cccccc}
			 & $\alpha\footnote{Note that our primed exponents are exponents above the critical temperature.}$ & $\alpha'$ & $\beta$ & $\gamma$ & $\gamma'$ \\
			 \hline 
			 \text{Analytic} & 0 & 0 & 0.125 & 1.75 & 1.75 \\
			 \text{MC 50} &  0.276(70) & 0.556(57) & 0.123(19) & 1.46(70) & 1.56(96)
		\end{tabular}
	\end{ruledtabular}
\end{table}

The values listed for $\alpha$ and $\alpha'$ are for completeness; the divergence is known to be a logarithmic divergence. However, it is satisfying that the exponents are not very large, corresponding to a relatively weak divergence. We obtain a very reasonable value for $\beta$ that is in-line with the analytic result. Unfortunately, the same cannot be said for $\gamma$ and $\gamma'$. There is a massive uncertainty such that we cannot determine a reasonably precise value for the exponents, only a range of possible values which does, indeed, contain the analytically correct value. From looking at the plot of the data, the large uncertainty is due to a combination of factors: significant scatter in the critical temperature region and relatively few points in that region that model the pseudo-divergent behavior. 

The fits for critical exponents also provided candidate values for the critical temperature, given by the parameter $b$ and $b'$. Averaging the values, we obtain a critical temperature of $T_c = 2.276(10)$, which contains the analytic value $T_c \approx 2.269$ in its uncertainty range. However, there are a few things to keep in mind when interpreting our $T_c$: first, that the critical temperature is a quantity that scales with the lattice size, so our determined value is really the critical temperature for the 2D Ising model on a $50 \times 50$ lattice. However, the scaling is weak \textbf{CITE}, so it is likely that the uncertainty captures this effect. Second, the specific heat's maximum does not lie exactly at the critical temperature for finite lattices, but the correction is very small, so we assume that the uncertainty makes this effect un-resolvable \textbf{CITE}. Finally, our temperature resolution is .01, so our value for $T_c$ is about as precise a result we could hope to get out of our simulation.

In summary, we have determined that our simulation with the Metropolis algorithm can successfully reproduce the low and high temperature limits of the ferromagnetic 2D Ising model, as well as generically evolve a lattice in a way that agrees with our intuition. It is also able to exhibit critical behavior at a critical temperature that roughly matches the analytically known critical temperature. Moreover, we are able to extract some information about a few critical exponents from the simulation. However, the extreme inefficiency of the Metropolis algorithm near the critical temperature region means that the confidence in our results is not particularly high. We conclude that our simulation of the 2D Ising model with the Metropolis algorithm is reasonably accurate, and limited primarily by the nature of the algorithm itself.






\section{Wolff Algorithm}
\subsection{Theory}
The second algorithm we use is the Wolff algorithm, a clustering algorithm named after Ulli Wolff. Cluster update algorithms have the advantage of being able to globally update the state of a system, whereas local update algorithms like the Metropolis algorithm can only operate on one variable at a time. In the Wolff algorithm, after choosing a random site, neighboring aligned spins are added with the probability $P_{\rm add}$. This process is repeated iteratively until no neighbors can be added to the cluster anymore, at which all the spins in the cluster are flipped simultaneously. 

If $m$ and $n$ represent non-added aligned spins to the cluster for the state before the flip ($A$) and the state after the flip ($B$), then the selection probability is 
\begin{equation}
\frac{g(X \to Y)}{g(Y \to X)} = (1-P_{\rm add})^{m-n}
\end{equation}
Detailed balance then implies that the acceptance ratios must satisfy:
\begin{equation}
(1-P_{\rm add})^{m-n} \frac{A(X \to Y)}{A(Y \to X)} = e^{-\beta(E_Y - E_X)}
\end{equation}

However, $E_Y - E_X = 2J(n-m)$, and thus
\begin{equation}
\frac{A(X \to Y)}{A(Y \to X)} = \left[ (1-P_{\rm add})e^{\beta J} \right]^{n-m}.
\end{equation}
By choosing $P_{\rm add} = 1-e^{2 \beta J}$, the acceptance probabilities become $A(X \to Y) = A(Y \to X) = 1$ and all the spins can be flipped once the cluster is formed. Therefore, with this special value of $P_{\rm add}$, the detailed balance condition is satisfied. Additionally, since any spin can clip, it is possible to reach any state from any other and thus this algorithm is ergodic as well. 

\subsection{Implementation}
Our simulation is performed entirely in Python. For all simulations, we take our spins to have value $\pm 1$ and use units such that $J = 1$. We define a 2D lattice with each element representing a spin; this lattice has periodic boundary conditions so as to neglect edge effects. The starting state for almost all of our simulations is the $T = 0$ state with all spins aligned.

Now we apply the Wolff algorithm. To do this, we use a program that performs the following steps:
\begin{enumerate}
	\item Randomly select a spin on the given lattice
	\item Add each neighboring aligned spin with the probability $P_{\rm add}$ to a cluster provided it is not already in the cluster
	\item Iteratively add all neighbors until no more can be added to the cluster
	\item Flip all the spins in the cluster at once
\end{enumerate}

We may then iterate over this function $n$ times to perform $n$ timesteps of evolution. Some pictures of the evolution of a lattice are shown in Fig.~\ref{fig:wolffevo}.
\begin{figure*}
	\includegraphics[width=\columnwidth]{"Wolff_evolution".pdf}
	\caption{\label{fig:wolffevo}The evolution of a $100 \times 100$ square lattice at $T = 2.4$ via the Wolff algorithm. The sequence is ordered from left to right, top to bottom. The two spin states $+1$ and $-1$ correspond to black and white, respectively. We have initialized the system in the $T = 0$ state, and plotted the system after 0, 1, 5, 10, 20, 50, 100, 500, and 1000 Monte Carlo steps. The lattice equilibrates much faster through the Wolff algorithm than through the Metropolis algorithm.}
\end{figure*}

\subsection{Optimizations}
Although the Wolff algorithm equilibrates faster near the critical temperature than the Metropolis algorithm does, the algorithm itself is not inherently fast. Thus a few optimizations were implemented, taking especial advantage of locality and runtime efficiency of certain data structures (similarly to the Metropolis algorithm when possible).

\subsection{Equilibration and Measurement}
Given that our initial lattice is at $T = 0$ or $T = \infty$, we have to allow the simulation to equilibrate at our desired temperature $T$ before taking any measurements. As with the Metropolis algorithm simulations, we take equilibration with the Wolff algorithm simulations to mean that thermodynamic quantities whose mean values depend on the system temperature are relatively stable within a narrow range. We consider 2 quantities: the total energy and the magnetization per spin. An example of how these quantities will evolve over time is shown in Fig.~\ref{fig:equil}. 
\begin{figure}
	\includegraphics[width=\columnwidth]{"Wolff_Equilibration1".pdf}
	\caption{\label{fig:equil}Plots of the total energy and the magnetization per spin on a $100 \times 100$ square lattice at $T = 2.7$ initialized at $T = \infty$ and evolved with the Wolff algorithm. We see that after roughly 5000 steps, the system reaches equilibrium, with the energy and magnetization per spin hovering around near $0$.}
\end{figure}

We see that equilibration occurs after roughly 5000 steps, where energy and magnetization per spin values start oscillating around a mean value. Additionally, since we are interested in a small range of temperatures, equilibration can start from the final state of the previous temperature iteration. Because the states will presumably be fairly close, equilibration occurs faster. After equilibration, we obtain mean values for various observables by taking a series of measurements at the same temperature, and then finding the arithmetic mean of the measurements. Since the Wolff algorithm is a globally updating algorithm, correlation is less of an issue than locally updating processes like the Metropolis algorithm. This fast equilibration, as well as the non-local method of updating, allows the Wolff algorithm to have much less pronounced fluctuations and slowing down near the critical temperature. 

\subsection{Error}
% The error in our measurements is computed using the bootstrap method -- the benefit of this method is that it does not require knowing the correlation time, nor does it require truly independent measurements. Moreover, it also readily applies to direct measurements, like mean energy, and to functions of means, like the specific heat.

\subsection{Accuracy Verification}
%As a first check, we want to make sure that the system behaves as we would expect at low and high temperature limits. In the low temperature limit, all the spins should be aligned so that the energy is at the minimum $E_0 = -2N$ and the magnitude of the magnetization is 1. In the high temperature limit, spins are randomly aligned so that both the energy and the magnetization are 0. Some examples on a $100 \times 100$ grid are shown in Figure \textbf{FIGURE}. We see that we have the expected behavior across the critical temperature, with a progression of total randomness to domain formation to full alignment going from high to low temperature, and the reverse going from low to high.
%
%A stronger check of the accuracy of simulation, and indeed our goal, is how well it can reproduce the expected critical behavior of the 2D Ising model. To do this, we explore a range of temperatures around the known critical temperature $T_c = \frac{2}{\ln(1+\sqrt{2})}J \approx 2.269J$ with $J = 1$. At every temperature value, we allow the system to equilibrate over $100L$ sweeps for a $L \times L$ lattice, and then proceed to make $100L$ measurements of the energy and magnetization, with each measurement separated by one sweep. The measurements can then be averaged to obtain the specific heat and magnetic susceptibility. All the plots of these values for every lattice we explored can be found in the \textbf{APPENDIX}.
%
%There are several caveats to keep in mind about our results. The equilibration time value is somewhat arbitrarily chosen using the heuristic that it takes order $L^3$ steps to equilibrate, and is probably not enough for temperatures very close to the critical temperature. However, we would like to explore intermediate-sized lattices, so we opt to trade accuracy in the computation for accuracy in the system size. From looking at the various plots for different lattice sizes, increasing the lattice size also greatly improves the precision of our results and reduces the error on each data point. The inaccuracy of the simulation near the critical temperature is particularly pronounced for the smaller lattices because the critical fluctuations affect a larger fraction of the lattice, and extracting information about the critical behavior is close to impossible due to the scatter of the data. Finally, the finite temperature resolution size ultimately limits the precision of our results, so we cannot really trust our results beyond 3 significant figures. Critical slowing down makes the Metropolis algorithm highly impractical for exploring finer temperature resolutions around the critical temperature, and the finite size of the lattice means a finer temperature resolution is unlikely to provide enough information to offset the extreme time cost.
%
%Using our values for the magnetization per spin and magnetic susceptibility, we perform a non-linear least squares fit to a function of the form
%\begin{equation}
%	f(x) = a\left(\frac{b-x}{b}\right)^c, \quad f'(x) = a'\left(\frac{x-b'}{b'}\right)^{c'}
%\end{equation}
%where the primed parameters are for fitting a curve above the critical temperature. We perform the fit using the SciPy \texttt{optimize.curve\_fit} function and use the \texttt{sigma} argument to manually input the errors computed using the bootstrap method. The fit parameters $b,b'$ and $c,c'$ correspond to the critical temperature and critical exponent, respectively. We can also fit the specific heat per spin to this form, but the analytic solution for the 2D Ising model implies a logarithmic divergence, rather than a power law. Furthermore, in the case of the specific heat, the maximum we see is actually at the critical temperature \textbf{CITE}. It is also for this reason that we are looking to explore the largest lattices we can that do not require excessive computational time. We also expect that the finite size of the lattice will affect the behavior of the various thermodynamic quantities around the phase transition; the divergences associated with the phase transition are only true divergences in the thermodynamic limit $N \to \infty$. It is also well-known that the critical exponents and critical temperature suffer from finite size effects. We have not attempted to analyze these effects carefully, but we keep them in mind when discussing the confidence in our results.


\subsection{Results}
The largest lattice we consider is a $30 \times 30$ lattice with measurements presented in Fig.~\ref{fig:WolffM}.
\begin{figure*}
	\includegraphics[width=\linewidth]{"Wolff_Measurements".pdf}
	\caption{\label{fig:WolffM}Plots showing the mean energy per spin, specific heat per spin, mean (absolute) magnetization per spin, magnetic susceptibility on a 30 $\times$ 30 lattice from $T = 1.6$ to $T = 2.9$ in increments of $\Delta T = .01$. 
		%The lines overlaid on the data are lines of best-of-fit determined using \texttt{optimize.curve\_fit}. 
		The equilibration time at each temperature is 30 steps and each data point is obtained from 150 measurements. The error bars computed using the bootstrap method are too small to see.}
\end{figure*}

As expected, we see evidence of critical behavior at roughly $T_c = 2.2$-$2.3$ with a second order phase transition. Near this critical temperature the magnetization per spin goes from 1 (below $T_c$) to 0 (above $T_c$). The peaks in the specific heat and susceptibility around $T_c$ remain finite throughout the simulation, and correspondingly, the energy and magnetization steps are not truly steps with an infinite derivative, though they are theoretically characterized by divergences. 

%Despite such effects, we are still able to see how the quantities generally behave in the critical temperature region, and our lattice is large enough that we have enough precision to attempt to fit curves with reasonable confidence. By fitting to functions with the form in Eq. (20), we find values for the critical exponents as presented in Table~\ref{tab:crit}. We separately discuss the obtained values for the critical temperature below.
%\begin{table}[b]%The best place to locate the table environment is directly after its first reference in text
%	\caption{\label{tab:crit}Various critical exponents obtained from applying the Wolff algorithm to a 30 $\times$ 30 grid. The uncertainties are obtained directly from the \texttt{curve\_fit} output covariance matrix .}
%	\begin{ruledtabular}
%		\begin{tabular}{cccccc}
%			 & $\alpha\footnote{Note that our primed exponents are exponents above the critical temperature.}$ & $\alpha'$ & $\beta$ & $\gamma$ & $\gamma'$ \\
%			 \hline 
%			 \text{Analytic} & 0 & 0 & 0.125 & 1.75 & 1.75 \\
%			 \text{MC 50} &  0.276(70) & 0.556(57) & 0.123(19) & 1.46(70) & 1.56(96)
%		\end{tabular}
%	\end{ruledtabular}
%\end{table}
%
%The values listed for $\alpha$ and $\alpha'$ are for completeness; the divergence is known to be a logarithmic divergence. However, it is satisfying that the exponents are not very large, corresponding to a relatively weak divergence. We obtain a very reasonable value for $\beta$ that is in-line with the analytic result. Unfortunately, the same cannot be said for $\gamma$ and $\gamma'$. There is a massive uncertainty such that we cannot determine a reasonably precise value for the exponents, only a range of possible values which does, indeed, contain the analytically correct value. From looking at the plot of the data, the large uncertainty is due to a combination of factors: significant scatter in the critical temperature region and relatively few points in that region that model the pseudo-divergent behavior. 
%
%The fits for critical exponents also provided candidate values for the critical temperature, given by the parameter $b$ and $b'$. Averaging the values, we obtain a critical temperature of $T_c = 2.276(10)$, which contains the analytic value $T_c \approx 2.269$ in its uncertainty range. However, there are a few things to keep in mind when interpreting our $T_c$: first, that the critical temperature is a quantity that scales with the lattice size, so our determined value is really the critical temperature for the 2D Ising model on a $50 \times 50$ lattice. However, the scaling is weak \textbf{CITE}, so it is likely that the uncertainty captures this effect. Second, the specific heat's maximum does not lie exactly at the critical temperature for finite lattices, but the correction is very small, so we assume that the uncertainty makes this effect un-resolvable \textbf{CITE}. Finally, our temperature resolution is .01, so our value for $T_c$ is about as precise a result we could hope to get out of our simulation.
%
In summary, the Wolff algorithm can successfully reproduce the low and high temperature limits of the ferromagnetic 2D Ising model, evolve a lattice in a way that agrees with intuition, and equilibrate much faster than locally updating algorithms such as the Metropolis algorithm. The critical temperature $T_c$ was reasonably estimated from the simulations, and behavior near $T_c$ roughly matches analytically known behavior. Inaccuracies mainly arise from runtime limitations as well as the finite size of the grid and the finite step size of the values.





\section{Potts Model}
\subsection{Theory}
A natural generalization of the Ising model is to consider spins with more possible states. Taking $q$ to be a positive integer, the $q$-state Potts model is defined by the Hamiltonian
\begin{equation}
	H = -J\sum_{\braket{ij}}\delta_{s_i,s_j} - h\sum_{i}s_i
\end{equation}
where $\delta_{ij}$ is the Kronecker delta which takes the value $1$ when $i = j$ and is 0 otherwise. As before, we will only consider the case of a vanishing external field and set $h = 0$. The Hamiltonian is again the sum of the energy in bonds between spins, with bonds between identical spins contributing $-J$ and bonds between non-identical spins contributing 0. By rewriting the Hamiltonian, we can see that the Ising model is equivalent to the $q = 2$ Potts model up to an energy rescaling and an additive constant:
\begin{equation}
	H = -\frac{1}{2}J\sum_{\braket{ij}}2\left(\delta_{s_i,s_j} - \frac{1}{2}\right) - \frac{1}{2}\sum_{\braket{ij}}J
\end{equation}

The $q$-state Potts model differs from the Ising model in a few crucial ways: there are now $q$-many ground states, corresponding to all spins on the lattice taking on the same value. Furthermore, the entropy of a Potts model with $q > 2$ is significantly higher at all temperatures, because there are far more possible microstates.

The order parameter in the Potts model also differs from the Ising model; it is a complex magnetization defined by:
\begin{equation}
	m = \frac{1}{N}\left|\sum_{k=1}^qe^{2\pi i k/q}n_k\right|
\end{equation}
where $n_k$ is the number of spins in state $k$. The magnetization of a particular spin is given by a generically complex $q$-th root of unity, and the total magnetization is then the magnitude of the sum of all the magnetizations. In the ordered phase, spins will preferentially align so that $n_k \to N$ for some particular $k \in [1,q]$, $n_j \to 0$ for all $j \neq k$, and $m \to 1$. In the disordered phase, we will have $m \to 0$ due to phase cancellation. Therefore $m$ is 0 below the critical temperature and is $1$ above it, as desired. Note that in the special case of $q = 2$, the roots of unity are $\pm 1$, so we recover the order parameter used in the Ising model.


\subsection{Implementation}
The Metropolis algorithm may still be applied to simulate the $q$-state Potts model: randomly select a spin $s_i$ and a value for the spin $s_i' \neq s_i$. Then compute the change in energy $\Delta E$ resulting from the change $s_i \to s_i'$. If $\Delta E < 0$, we accept the new state. If $\Delta E > 0$, we accept the new state with probability $e^{-\beta \Delta E}$. All of our previous optimization comments remain true: one need only compute the energy of the full lattice at the beginning of the simulation and increment by $\Delta E$ at each step, because the change in energy of flipping one spin is determined only by the nearest neighbors of the spin. Moreover, the spectrum of possible $\Delta E$ is still small: there are only 4 possible non-zero energy changes. Then one can again just compute the Boltzmann factors at the beginning of the simulation.

While the Metropolis algorithm is reasonably efficient for low $q$, it significantly slows down as $q$ increases. For example, for $q = 100$ and low temperatures, consider a spin $s_i$ whose neighbors all have different values. There are at most 4 values of $s_i'$ that will decrease the energy of the lattice, so there is at most a $4/99$ chance that the algorithm will select a spin value that lowers the energy. Because the other $95/99$ choices for $s_i'$ have $\Delta E = 0$, they will always be accepted. The algorithm will spend most of the time accepting new configurations that do not give rise to any dynamics, because the ratio of interesting states to possible states is so high. On the other hand, suppose $s_i$ does match one of its neighbors. Then there are at $96/99$ choices for $s_i'$ that yield $\Delta E > 0$, and at low temperatures, the acceptance ratio is very low. The algorithm will spend most of its time rejecting these higher energy states. 

A more efficient algorithm for higher $q$ is the heat-bath algorithm, which assigns chooses states as if in a heat bath at the given temperature. More precisely, the acceptance ratio is 1 for every state, while the selection probability for each state $n \in \{1,\ldots,q\}$ is
\begin{equation}
	p_n = \frac{e^{-\beta E_n}}{\sum_{i=1}^q e^{-\beta E_i}}
\end{equation}
This manifestly satisfies detailed balance, and will preferentially select states that evolve the system, rather than spend a significant amount of time sorting through un-dynamical states. The algorithm can then be implemented in the following steps:
\begin{enumerate}
	\item Random select a spin on a given lattice
	\item Choose a new value $n$ for the spin based on the probability distribution $p_n$ defined above
	\item Return the lattice with the new value for the spin
\end{enumerate}

The selection probability distribution also has the advantage of being relatively easy and efficient to implement. The Hamiltonian can be split into a two parts: one that depends on the spin in question $s_k$, and one that does not.
\begin{equation}
	H = -J\sum_{\braket{ij}\ i,j\neq k}\delta_{s_i,s_j} - J\sum_{\braket{ik}}\delta_{s_i,s_k}
\end{equation}
The first term will be the same in all the exponentials in $p_n$, so it cancels out. We therefore need only sum over 4 terms for each exponential. Moreover, we do not actually need to compute the energy in every case; clearly there can only be at most 4 values of $s_k$ that will yield a non-zero contribution to the Hamiltonian. Therefore, there are only 4 exponentials in $p_n$ that actually need to be computed, with the rest just having value 1. In computing the energy for those 4 exponentials, there are only 4 non-zero values for $\Delta E$: $\{-J, -2J, -3J, -4J\}$. Because the spectrum is so small, we can use the same trick as in our other simulations, where we compute the exponentials once at the beginning of the simulation and just pull the values as needed.

A comparison of the equilibration times for the Metropolis and heat-bath algorithms for the $q = 10$ model is shown in Fig.~ \ref{fig:pottsequil}.
\begin{figure}
	\includegraphics[width=\columnwidth]{"potts_equil".pdf}
	\caption{\label{fig:pottsequil}The energy per spin of a $q = 10$ (top) and $q = 2$ (bottom) Potts grid at $T = 0.4$ and initialized to a random $T = \infty$ state. The orange line (lower line in the top, upper line in the bottom) is the energy of the grid evolved with the Metropolis algorithm, and the blue line is the energy of the grid evolved with the heat-bath algorithm.}
\end{figure}

The lattice evolved by the heat-bath algorithm equilibrates after around 300 sweeps, while the lattice evolved by the Metropolis algorithm still has not equilibrated after 1000 sweeps. Although one step of the heat-bath algorithm is slower than the Metropolis algorithm by a $\mathcal{O}(1)$ factor (about 2.5 times faster in the generation of the plot), the equilibration time is faster by a factor of at least around 4, representing a gain in efficiency. Naturally the disparity grows as $q$ gets larger. The gain in efficiency is also important when exploring a range of temperatures, because the efficiency gain multiplies against the number of times one must equilibrate at a new temperature.

However, for small $q$, when there are only a few un-dynamical states, the Metropolis algorithm may be faster. As an example, we show equilibration times for a $q = 2$ model, as can be seen in the lower plot of Fig.~\ref{fig:pottsequil}. The Metropolis algorithm not only equilibrates faster, but each steps is also faster than the heat-bath algorithm, so it is clearly faster overall. In fact, this is not too surprising since the $q = 2$ Potts model is equivalent to the 2D Ising model, and the Metropolis algorithm is generally accepted as the fastest single-spin algorithm for simulating the 2D Ising model. 

\subsection{Results}
We simulate the Potts model for $q = 2,\ 3,\ 4,\ 5,\ 10,$ and $50$ on a $25 \times 25$ lattice, using thee Metropolis algorithm for $q = 2, 3, 4, 5$, and the heat-bath algorithm for $q = 10$ and $q = 50$. The results of all the simulations may be found in the \textbf{APPENDIX}. We analyze the critical behavior of the $q = 3$ model, and then comment on the general characteristics of the critical behavior of all the simulations.







\section{XY Model}
Whereas the Ising model represents a simple system of spins constrained to 1 dimension ($n=1$), the XY model represents a system of with interacting two-dimensional vector spins ($n=2$). The 2D XY model on a lattice is defined by the Hamiltonian: 
\begin{equation}
H = -J\sum_{\braket{ij}} \cos(\theta_i - \theta_j) - h\sum_i \theta_i
\end{equation}
where $\theta_i$ is the $i$th spin on the lattice, $\braket{ij}$ refers to nearest neighbors on the lattice, $J$ is the coupling strength between the spins, and $h$ is a coupling to an external magnetic field $\textbf{CITE}$. The state $\theta_i$ can belong to the region [0, 2$\pi$). Again, without loss of generality, we work with the basic case of the square lattice with the case of no external magnetic field ($h=0$). When $J > 0$, the model is ferromagnetic, and when $J<0$, the model is anti-ferromagnetic. 

This system is of great theoretical interest because of its circular symmetry. As a result of the Mermin--Wagner Theorem, in this $n \leq 2$ system, continuous symmetries cannot be spontaneously broken at finite temperatures with short-range interactions within the system $\textbf{CITE}$. However, although there is no second-order phase transition (the derivative of the specific heat per spin $c_v$ is continuous), there is a transition from a high-temperature disordered phase to a low-temperature quasi-ordered phase. In the former disordered phase, the correlation function decays exponentially; in the latter quasi-ordered phase, most of the spins tend to be aligned and the correlation function has a power law decay---the exponential correlation length is infinite. This transition between the two phases, called the \textit{Kosterlitz--Thouless transition} (hereafter the KT transition), occurs at some critical temperature $T_c$ and is of infinite order $\textbf{CITE}$. 

The degrees of freedom determining how ordered the system is come from topologically stable configurations in the 2D XY model called \textit{vortices}, which can be visualized as local sources/sinks (vortices) and saddles (anti-vortices) of multiple spins $\textbf{FIGURES}$. In other words, these vortices are points on the lattice around which the spins are configured in a winding pattern, clockwise or counterclockwise. Below $T_c$, there are only a few vortices, which present themselves in bound vortex-antivortex pairs of net zero vorticity $\textbf{FIGURE}$. Above $T_c$, there are many free vortices present in what can be called a vortex plasma. Vortex generation has been found to be the reason behind the exponential correlation decay at the high-temperature disordered phase $\textbf{CITE}$. Since these vortices are not bound, they can move due to the influence of an applied magnetic field. 

Thus, the KT transition occurs because of vortex-antivortex pairs unbinding at $T_c$, which causes a sudden change in the response form to an arbitrarily weak applied magnetic field. This transition is observable in simulations of the 2D XY model, though large lattice sizes and long simulation times are often necessary to simulate and observe the transition accurately. The 2D XY model can be used to model systems that possess similar properties of symmetry and exhibit similar transition behavior, such as superfluidity in thin films---observed experimentally with He$^4$ $\textbf{CITE}$.

\subsection{Implementation}
The Metropolis algorithm may again be used to simulate the 2D XY model. For all simulations, the starting states consist of randomly arranged spins on a lattice with periodic boundary conditions, with the option of seeding available for result reproducibility. As with the Metropolis algorithm implementation for the 2D Ising model simulation, we implement a program in Python that follows the following steps:
\begin{enumerate}
	\item Randomly select a spin on the given lattice
	\item Change the spin to a random orientation and compute the change in Hamiltonian energy of the lattice $\Delta H$
	\item If $\Delta H < 0$, accept the new state
	\item If $\Delta H > 0$, generate a random number $X$ from a uniform distribution $[0,1)$
	\item If $X < e^{-\beta \Delta H}$, accept the new state; otherwise reject the new state
\end{enumerate}
As before, the energy of a spin on the lattice is a local property with four neighboring spins affected. This algorithm is iterated for $N$ units of time, with $n$ steps of evolution per sweep.

A Wolff clustering algorithm was considered, but due to time constraints was unable to be implemented. However, such an algorithm for the 2D XY model would have been implemented similarly to the Wolff clustering algorithm implemented for the 2D Ising model. A random spin on the lattice $\vv \theta_i$ would be chosen, as well as a random two-dimensional unit vector $\vv r$. Then the clustering condition, rather than having the spins be the same, would be the condition $(\vv \theta_i \cdot \vv r) (\vv \theta_i \cdot \vv r) > 0$, after which the spin would be accepted to cluster with some probability. The cluster would then be flipped by reflecting every spin in the cluster through the line perpendicular to $\vv r$. Such a clustering algorithm implementation would greatly speed up the 2D XY model simulation at low temperatures, enabling the simulation to reach an equilibrium configuration faster than the Metropolis algorithm used. 

\section{Conclusion}
\textbf{HOW TO IMPROVE CODE FOR FUTURE (efficiency, flexible equilibration times, correlation times etc.)}

\section{Division of Labor}


\section{Acknowledgments}
We gratefully acknowledge the assistance of our teaching assistant Edwin Huang, who advised this project, answered many a question and email, reviewed parts of our code, and many other things too numerous to list here. We also thank Professor Blas Cabrera for his teaching and his work in putting together the new iteration of PHYSICS 113.


\section{Appendix}






\end{document}